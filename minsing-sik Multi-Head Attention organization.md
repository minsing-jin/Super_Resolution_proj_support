# Minsing-sik Multi-head attention 정리

## 결론
##### multi-head attention

이미지 처리 분야에서 attention 활용

## 선행지식(prior knowledge)
### 1. Attention이란 무엇인가
#### 어텐션 메커니즘

### 2. 용어 설명
1. embedding이란?<br>
   NLP에서 embedding은 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자 형태인 vector로 바꾼 결과/ 일련의 과정 
   전체<br>
2. NLP(Natural Language Processing)
   자연어 처리
3. LSTM(Long short term memory)
   장기/단기 기억을 가능하게 설계한 신경망 구조. 기존 RNN이 출력과 먼위치에 있는 정보를 기억할 수 없고, 장기간의 패턴을 학습하는데에 제한되는 단점 보완, 자세한 설명은 아래 설명 참조
Reference: http://www.incodom.kr/LSTM


https://wikidocs.net/22893
https://www.grainpowder.net/posts/dl/nlp/vaswani2017/
https://gist.github.com/ihoneymon/652be052a0727ad59601
https://gist.github.com/ihoneymon/652be052a0727ad59601
https://heung-bae-lee.github.io/2020/01/16/NLP_01/
https://wikidocs.net/24996
https://wikidocs.net/22886
