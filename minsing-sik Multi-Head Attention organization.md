# Minsing-sik Multi-head attention 정리

## 결론
##### multi-head attention

이미지 처리 분야에서 attention 활용

## 선행지식(prior knowledge)
### 1. Attention이란 무엇인가

### 2. 용어 설명
1. embedding이란?<br>
   NLP에서 embedding은 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자 형태인 vector로 바꾼 결과/ 일련의 과정 
   전체<br>
2. NLP(Natural Language Processing)
   자연어 처리
3. LSTM(Long short term memory)
   장기/단기 기억을 가능하게 설계한 신경망 구조. 기존 RNN이 출력과 먼위치에 있는 정보를 기억할 수 없다는 단점 보완
   ![image](https://user-images.githubusercontent.com/60510718/178132038-2be8f6e2-dde8-43d6-9280-61ea7155370e.png)
